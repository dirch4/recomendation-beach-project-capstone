{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B6MgeviJBkTb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import joblib # Untuk menyimpan/memuat model\n",
        "import numpy as np # Untuk operasi array seperti argsort\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Untuk TensorFlow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdmfJaExgPsK",
        "outputId": "b0a9a7bd-3249-4ef5-fb38-886fd7cd0099"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-06-03 23:29:27.872057: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748968167.885461  166633 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748968167.889397  166633 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1748968167.899315  166633 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748968167.899329  166633 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748968167.899330  166633 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748968167.899331  166633 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-06-03 23:29:27.902550: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Inisialisasi NLTK Stopwords dan Sastrawi Stemmer ---\n",
        "# Pastikan Anda telah mendownload stopwords. Jika belum, uncomment baris di bawah dan jalankan sekali:\n",
        "import nltk\n",
        "import ssl\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words_id = set(stopwords.words('indonesian'))\n",
        "\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "stemmer_factory = StemmerFactory()\n",
        "stemmer = stemmer_factory.create_stemmer()\n",
        "\n",
        "print(\"Memulai proses persiapan data dan pelatihan model...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERNx_esUgbu2",
        "outputId": "2121b4d7-1338-4ad8-d9b2-a292e1728c04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai proses persiapan data dan pelatihan model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/crxtan/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Fungsi Pra-pemrosesan Teks ---\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words_id]\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Fungsi Standarisasi Nama untuk Penggabungan\n",
        "def standardize_name_for_merge(name):\n",
        "    if pd.isna(name):\n",
        "        return ''\n",
        "    name = str(name).lower()\n",
        "    name = re.sub(r'[^\\w\\s]', '', name)\n",
        "    name = re.sub(r'\\s+', ' ', name).strip()\n",
        "    return name\n"
      ],
      "metadata": {
        "id": "0H3l_Tqskm6A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BAGIAN 1: MUAT DAN PRA-PEMROSESAN DATASET ---\n",
        "\n",
        "# 1.1 Muat Dataset Pertama (Cleaned_data.csv) menjadi df\n",
        "try:\n",
        "    df = pd.read_csv('Downloads/Cleaned_data.csv')\n",
        "    print(\"\\nDataset 'Cleaned_data.csv' berhasil dimuat ke df.\")\n",
        "\n",
        "    # Penanganan nilai hilang di kolom teks yang akan digabungkan\n",
        "    df['name'].fillna('', inplace=True)\n",
        "    df['description'].fillna('', inplace=True)\n",
        "    df['review_keywords'].fillna('', inplace=True)\n",
        "    df['address'].fillna('', inplace=True) # Untuk standarisasi place_name gabungan\n",
        "\n",
        "    # Buat kolom 'content' gabungan awal untuk Content-Based Filtering\n",
        "    df['content'] = df['name'] + ' ' + \\\n",
        "                            df['description'] + ' ' + \\\n",
        "                            df['review_keywords']\n",
        "    df['processed_content'] = df['content'].apply(preprocess_text)\n",
        "\n",
        "    print(\"Kolom konten awal untuk df berhasil diproses.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Pastikan file 'Cleaned_data.csv' ada di direktori yang sama.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Terjadi kesalahan saat memproses 'Cleaned_data.csv': {e}\")\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtDK3mcykq2U",
        "outputId": "79f5c298-8ec0-4952-99dd-8410911879c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_166633/364062184.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['name'].fillna('', inplace=True)\n",
            "/tmp/ipykernel_166633/364062184.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['description'].fillna('', inplace=True)\n",
            "/tmp/ipykernel_166633/364062184.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['review_keywords'].fillna('', inplace=True)\n",
            "/tmp/ipykernel_166633/364062184.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['address'].fillna('', inplace=True) # Untuk standarisasi place_name gabungan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset 'Cleaned_data.csv' berhasil dimuat ke df.\n",
            "Kolom konten awal untuk df berhasil diproses.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2 Muat Dataset Kedua (SentimentReview.csv) menjadi df_review\n",
        "try:\n",
        "    df_review = pd.read_csv('Downloads/SentimentReview.csv')\n",
        "    print(\"\\nDataset 'SentimentReview.csv' berhasil dimuat ke df_review.\")\n",
        "\n",
        "    # Konversi 'published_at_date' ke datetime\n",
        "    df_review['published_at_datetime'] = pd.to_datetime(df_review['published_at_date'])\n",
        "\n",
        "    # Pastikan 'is_local_guide' menjadi integer (0/1)\n",
        "    df_review['is_local_guide'] = df_review['is_local_guide'].fillna(0).astype(int)\n",
        "\n",
        "    # Isi NaN pada 'text_akhir' dengan string kosong (teks yang sudah diproses)\n",
        "    df_review['text_akhir'].fillna('', inplace=True)\n",
        "\n",
        "    print(\"Kolom tanggal, is_local_guide, dan text_akhir di df_review berhasil diproses.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Pastikan file 'SentimentReview.csv' ada di direktori yang sama.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Terjadi kesalahan saat memproses 'SentimentReview.csv': {e}\")\n",
        "    exit()\n",
        "\n",
        "print(\"\\nKedua dataset telah dimuat dan diproses secara awal.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LxuV9rUku0w",
        "outputId": "c88d9536-ecb9-4443-a7c0-08f687e2e51d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset 'SentimentReview.csv' berhasil dimuat ke df_review.\n",
            "Kolom tanggal, is_local_guide, dan text_akhir di df_review berhasil diproses.\n",
            "\n",
            "Kedua dataset telah dimuat dan diproses secara awal.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_166633/581497968.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_review['text_akhir'].fillna('', inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BAGIAN 2: PENGGABUNGAN DATASET BERDASARKAN place_name ---\n",
        "\n",
        "print(\"\\nMelakukan standarisasi 'place_name' dan membuat kunci gabungan untuk merge...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiL4b9rmkxBX",
        "outputId": "7da960c3-3c29-4221-ef1a-9cd05a89967d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Melakukan standarisasi 'place_name' dan membuat kunci gabungan untuk merge...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 Standarisasi di df (Dataset Pertama)\n",
        "df['name_standardized'] = df['name'].apply(standardize_name_for_merge)\n",
        "df['address_standardized'] = df['address'].apply(standardize_name_for_merge)\n",
        "df['merge_key'] = df['name_standardized'] + '_' + df['address_standardized']"
      ],
      "metadata": {
        "id": "_kRfTv98ky9Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 Standarisasi di df_review (Dataset Kedua)\n",
        "df_review['place_name_standardized'] = df_review['place_name'].apply(standardize_name_for_merge)\n",
        "df_review['merge_key'] = df_review['place_name_standardized'] # Kunci gabungan hanya place_name\n",
        "\n",
        "print(\"Standarisasi nama dan pembuatan kunci gabungan selesai.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrvkAjywk0uj",
        "outputId": "7dea1b8e-d390-4f41-cb87-2e8b149ce75e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standarisasi nama dan pembuatan kunci gabungan selesai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3 Agregasi data ulasan dari df_review per tempat wisata\n",
        "df_reviews_agg = df_review.groupby('merge_key').agg(\n",
        "    avg_review_rating=('rating', 'mean'),\n",
        "    total_reviews_from_raw=('rating', 'count'),\n",
        "    avg_polarity_score=('polarity_score', 'mean'),\n",
        "    positive_reviews_count=('polarity', lambda x: (x == 'positive').sum()),\n",
        "    negative_reviews_count=('polarity', lambda x: (x == 'negative').sum()),\n",
        "    all_review_texts=('text_akhir', lambda x: ' '.join(x.dropna().astype(str)))\n",
        ").reset_index()\n",
        "\n",
        "print(\"Dataset ulasan diagregasi per tempat wisata.\")\n",
        "print(df_reviews_agg.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYPmZP1ek2oQ",
        "outputId": "94420c2e-0314-4383-a7e3-1031c7d62979"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ulasan diagregasi per tempat wisata.\n",
            "                   merge_key  avg_review_rating  total_reviews_from_raw  \\\n",
            "0  101 nusa lima beach resto           4.084507                      71   \n",
            "1         adhi pradana beach           4.428571                      14   \n",
            "2             agal waterfall           4.818182                      44   \n",
            "3     agro wisata dream land           3.412281                     114   \n",
            "4          air terjun bintan           4.147239                     163   \n",
            "\n",
            "   avg_polarity_score  positive_reviews_count  negative_reviews_count  \\\n",
            "0            1.436620                      53                      18   \n",
            "1           -1.071429                       7                       7   \n",
            "2           -1.363636                      31                      13   \n",
            "3           -1.535088                      57                      57   \n",
            "4           -0.822086                     104                      59   \n",
            "\n",
            "                                    all_review_texts  \n",
            "0  wisata pemandangan hamparan laut yg luas terse...  \n",
            "1  bagus pantai sayang pengelolaan sampah pondok ...  \n",
            "2  air terjun agal desa marenteh kecamatan alas s...  \n",
            "3  segi jalan lokasi bnyak jalan jelek kasi tau a...  \n",
            "4  salah favorit refreshing gunung bintan jalur y...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.4 Gabungkan df dengan df_reviews_agg\n",
        "df_merged = pd.merge(df, df_reviews_agg, on='merge_key', how='left')\n",
        "\n",
        "print(\"\\nPenggabungan dataset selesai.\")\n",
        "print(\"Informasi df_merged setelah penggabungan:\")\n",
        "df_merged.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PAqtVU3k4fu",
        "outputId": "1a2dcb31-c7d8-4cd1-884d-4a54d92c7636"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Penggabungan dataset selesai.\n",
            "Informasi df_merged setelah penggabungan:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2392 entries, 0 to 2391\n",
            "Data columns (total 21 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   place_id                2392 non-null   object \n",
            " 1   name                    2392 non-null   object \n",
            " 2   description             2392 non-null   object \n",
            " 3   reviews                 2392 non-null   int64  \n",
            " 4   rating                  2392 non-null   float64\n",
            " 5   featured_image          2391 non-null   object \n",
            " 6   address                 2392 non-null   object \n",
            " 7   review_keywords         2392 non-null   object \n",
            " 8   link                    2392 non-null   object \n",
            " 9   coordinates             2392 non-null   object \n",
            " 10  content                 2392 non-null   object \n",
            " 11  processed_content       2392 non-null   object \n",
            " 12  name_standardized       2392 non-null   object \n",
            " 13  address_standardized    2392 non-null   object \n",
            " 14  merge_key               2392 non-null   object \n",
            " 15  avg_review_rating       0 non-null      float64\n",
            " 16  total_reviews_from_raw  0 non-null      float64\n",
            " 17  avg_polarity_score      0 non-null      float64\n",
            " 18  positive_reviews_count  0 non-null      float64\n",
            " 19  negative_reviews_count  0 non-null      float64\n",
            " 20  all_review_texts        0 non-null      object \n",
            "dtypes: float64(6), int64(1), object(14)\n",
            "memory usage: 392.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.5 Finalisasi Fitur Konten untuk Content-Based Model\n",
        "# Isi NaN dari kolom baru setelah merge\n",
        "df_merged['avg_review_rating'].fillna(df_merged['avg_review_rating'].mean(), inplace=True)\n",
        "df_merged['total_reviews_from_raw'].fillna(0, inplace=True)\n",
        "df_merged['avg_polarity_score'].fillna(0, inplace=True)\n",
        "df_merged['positive_reviews_count'].fillna(0, inplace=True)\n",
        "df_merged['negative_reviews_count'].fillna(0, inplace=True)\n",
        "df_merged['all_review_texts'].fillna('', inplace=True)\n",
        "\n",
        "# Perbarui 'final_processed_content' (teks dari df + teks dari ulasan yang sudah bersih)\n",
        "df_merged['final_processed_content'] = df_merged['processed_content'] + ' ' + df_merged['all_review_texts']\n",
        "\n",
        "print(\"\\nFitur konten akhir (final_processed_content) untuk Content-Based model siap.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqXN-zUqk474",
        "outputId": "f780aa59-e5b5-403f-ff3e-2d7ad55a8301"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fitur konten akhir (final_processed_content) untuk Content-Based model siap.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_166633/1700334273.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_merged['avg_review_rating'].fillna(df_merged['avg_review_rating'].mean(), inplace=True)\n",
            "/tmp/ipykernel_166633/1700334273.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_merged['total_reviews_from_raw'].fillna(0, inplace=True)\n",
            "/tmp/ipykernel_166633/1700334273.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_merged['avg_polarity_score'].fillna(0, inplace=True)\n",
            "/tmp/ipykernel_166633/1700334273.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_merged['positive_reviews_count'].fillna(0, inplace=True)\n",
            "/tmp/ipykernel_166633/1700334273.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_merged['negative_reviews_count'].fillna(0, inplace=True)\n",
            "/tmp/ipykernel_166633/1700334273.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_merged['all_review_texts'].fillna('', inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BAGIAN 3: PELATIHAN DAN PENYIMPANAN MODEL CONTENT-BASED ---\n",
        "\n",
        "print(\"\\nMelatih dan menyimpan model Content-Based...\")\n",
        "\n",
        "# 3.1 Inisialisasi dan latih TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.85)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df_merged['final_processed_content'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_PTItc2k7ZR",
        "outputId": "ed122e7a-dc62-45ad-9186-29e2be35138c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Melatih dan menyimpan model Content-Based...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.2 Hitung Cosine Similarity Matrix\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)"
      ],
      "metadata": {
        "id": "BrvPVl1Uk8H-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.3 Buat mapping place_id ke index\n",
        "indices = pd.Series(df_merged.index, index=df_merged['place_id']).drop_duplicates()\n"
      ],
      "metadata": {
        "id": "mCDOrFxDlA2n"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.4 Simpan Model Content-Based dan metadata yang diperlukan\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "joblib.dump(cosine_sim_matrix, 'cosine_sim_matrix.pkl')\n",
        "joblib.dump(indices, 'place_indices_map.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2irnwfKwlH7N",
        "outputId": "36d3d7f8-e1df-46dc-95c6-5360e3b713bd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['place_indices_map.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PERBAIKAN DI SINI: HAPUS 'polarity' dari daftar kolom untuk df_places_metadata ---\n",
        "df_merged_metadata = df_merged[['place_id', 'name', 'description', 'reviews', 'rating', 'featured_image',\n",
        "                                'address', 'review_keywords', 'link', 'coordinates', 'avg_review_rating',\n",
        "                                'total_reviews_from_raw', 'avg_polarity_score',\n",
        "                                'positive_reviews_count', 'negative_reviews_count', 'final_processed_content', 'merge_key']]\n",
        "df_merged_metadata.to_pickle('df_places_metadata.pkl')\n",
        "\n",
        "print(\"Model Content-Based (TF-IDF Vectorizer, Cosine Similarity Matrix) dan metadata tempat wisata telah disimpan.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtEoEFoelKIe",
        "outputId": "e8b9a300-2a36-4d3a-8e9c-ca1bcffdbd70"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Content-Based (TF-IDF Vectorizer, Cosine Similarity Matrix) dan metadata tempat wisata telah disimpan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BAGIAN 4: PERSIAPAN DAN PELATIHAN MODEL COLLABORATIVE FILTERING (TensorFlow Matrix Factorization) ---\n",
        "\n",
        "print(\"\\nMenyiapkan dan melatih model Collaborative Filtering (TensorFlow Matrix Factorization)...\")\n",
        "\n",
        "# 4.1 Siapkan data untuk TensorFlow\n",
        "cf_data_tf = df_review[['user_id', 'merge_key', 'rating']].copy()\n",
        "cf_data_tf.dropna(subset=['rating'], inplace=True)\n",
        "\n",
        "# Map user_id dan item_id (merge_key) ke integer kontigu\n",
        "user_ids = cf_data_tf['user_id'].unique()\n",
        "item_ids = cf_data_tf['merge_key'].unique()\n",
        "\n",
        "num_users = len(user_ids)\n",
        "num_items = len(item_ids)\n",
        "\n",
        "user_id_mapping = {id: i for i, id in enumerate(user_ids)}\n",
        "item_id_mapping = {id: i for i, id in enumerate(item_ids)}\n",
        "\n",
        "# Buat DataFrame dengan ID integer yang dipetakan\n",
        "cf_data_tf['user_encoded'] = cf_data_tf['user_id'].map(user_id_mapping)\n",
        "cf_data_tf['item_encoded'] = cf_data_tf['merge_key'].map(item_id_mapping)\n",
        "\n",
        "# Pisahkan data menjadi fitur (input) dan target (rating)\n",
        "x = cf_data_tf[['user_encoded', 'item_encoded']].values\n",
        "y = cf_data_tf['rating'].values\n",
        "\n",
        "# Pisahkan data menjadi training dan testing set\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwm4_bGzlUHW",
        "outputId": "8d5639f7-e52b-497c-8ebb-72aa5df0bf4e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Menyiapkan dan melatih model Collaborative Filtering (TensorFlow Matrix Factorization)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.2 Definisikan Model Matrix Factorization dengan Keras\n",
        "embedding_size = 50 # Ukuran embedding (bisa disesuaikan)\n",
        "\n",
        "user_input = keras.Input(shape=(1,), name='user_id')\n",
        "item_input = keras.Input(shape=(1,), name='item_id')\n",
        "\n",
        "# Embedding layer untuk user\n",
        "user_embedding = layers.Embedding(\n",
        "    input_dim=num_users,\n",
        "    output_dim=embedding_size,\n",
        "    input_length=1,\n",
        "    name='user_embedding'\n",
        ")(user_input)\n",
        "user_vec = layers.Flatten(name='user_vector')(user_embedding)\n",
        "\n",
        "# Embedding layer untuk item\n",
        "item_embedding = layers.Embedding(\n",
        "    input_dim=num_items,\n",
        "    output_dim=embedding_size,\n",
        "    input_length=1,\n",
        "    name='item_embedding'\n",
        ")(item_input)\n",
        "item_vec = layers.Flatten(name='item_vector')(item_embedding)\n",
        "\n",
        "# Dot product dari embedding user dan item untuk prediksi rating\n",
        "dot_product = layers.Dot(axes=1, name='dot_product')([user_vec, item_vec])\n",
        "\n",
        "# Optional: Tambahkan bias untuk user dan item (membantu akurasi)\n",
        "user_bias = layers.Embedding(num_users, 1, name='user_bias')(user_input)\n",
        "item_bias = layers.Embedding(num_items, 1, name='item_bias')(item_input)\n",
        "dot_product_with_bias = layers.Add()([dot_product, layers.Flatten()(user_bias), layers.Flatten()(item_bias)])\n",
        "\n",
        "# Output layer (aktivasi sigmoid untuk skala rating 1-5, lalu scaling)\n",
        "# Jika rating asli 1-5, sigmoid output 0-1, maka dikalikan (MAX_RATING - MIN_RATING) dan ditambah MIN_RATING\n",
        "min_rating = 1.0 # Sesuaikan dengan rating minimum Anda\n",
        "max_rating = 5.0 # Sesuaikan dengan rating maksimum Anda\n",
        "\n",
        "output = layers.Activation('sigmoid')(dot_product_with_bias) # Output range 0-1\n",
        "output = layers.Lambda(lambda x: x * (max_rating - min_rating) + min_rating)(output) # Scale to min_rating-max_rating\n",
        "\n",
        "model_cf = keras.Model(inputs=[user_input, item_input], outputs=output)\n",
        "\n",
        "# Kompilasi model (gunakan Mean Squared Error sebagai loss untuk rating)\n",
        "model_cf.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E3K-v_GldUr",
        "outputId": "660f9cab-0b3c-43b5-f238-a39c00d610e2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/crxtan/.pyenv/versions/3.12.10/envs/ML/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n",
            "I0000 00:00:1748968521.005213  166633 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8868 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.3 Latih Model\n",
        "history = model_cf.fit(\n",
        "    [x_train[:, 0], x_train[:, 1]], # Input user_encoded, item_encoded\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=10, # Sesuaikan jumlah epoch\n",
        "    validation_data=([x_val[:, 0], x_val[:, 1]], y_val),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 4.4 Simpan Model TensorFlow\n",
        "model_cf.save('cf_tf_matrix_factorization_model.h5')\n",
        "\n",
        "# Simpan juga mapping ID\n",
        "joblib.dump(user_id_mapping, 'user_id_mapping.pkl')\n",
        "joblib.dump(item_id_mapping, 'item_id_mapping.pkl')\n",
        "joblib.dump(user_ids, 'unique_user_ids.pkl') # Simpan daftar user ID asli\n",
        "joblib.dump(item_ids, 'unique_item_ids.pkl') # Simpan daftar item ID asli (merge_key)\n",
        "\n",
        "print(\"Model Collaborative Filtering (TensorFlow) telah disimpan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1BOWnIClgo_",
        "outputId": "c3388a56-4abd-4e84-f3fc-495108455786"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1748968536.900706  169800 service.cc:152] XLA service 0x7c5f64002280 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1748968536.900722  169800 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
            "2025-06-03 23:35:36.928375: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "I0000 00:00:1748968537.009292  169800 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m  60/3762\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 2.8264"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "I0000 00:00:1748968537.308231  169800 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3762/3762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 2.4676 - val_loss: 1.6241\n",
            "Epoch 2/10\n",
            "\u001b[1m3762/3762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 1.1895 - val_loss: 1.2354\n",
            "Epoch 3/10\n",
            "\u001b[1m3762/3762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 0.3372 - val_loss: 1.1609\n",
            "Epoch 4/10\n",
            "\u001b[1m3762/3762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 0.0917 - val_loss: 1.1679\n",
            "Epoch 5/10\n",
            "\u001b[1m3762/3762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0624 - val_loss: 1.0891\n",
            "Epoch 6/10\n",
            "\u001b[1m3762/3762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0459 - val_loss: 1.1404\n",
            "Epoch 7/10\n",
            "\u001b[1m3762/3762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0369 - val_loss: 1.0911\n",
            "Epoch 8/10\n",
            "\u001b[1m3762/3762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0343 - val_loss: 1.1513\n",
            "Epoch 9/10\n",
            "\u001b[1m3762/3762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 0.0290 - val_loss: 1.1055\n",
            "Epoch 10/10\n",
            "\u001b[1m3762/3762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 0.0268 - val_loss: 1.1652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Collaborative Filtering (TensorFlow) telah disimpan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import joblib # Untuk menyimpan/memuat model\n",
        "import numpy as np # Untuk operasi array seperti argsort\n",
        "\n",
        "# Untuk TensorFlow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# --- Inisialisasi NLTK Stopwords dan Sastrawi Stemmer ---\n",
        "# Pastikan Anda telah mendownload stopwords. Jika belum, uncomment baris di bawah dan jalankan sekali:\n",
        "# import nltk\n",
        "# import ssl\n",
        "# try:\n",
        "#     _create_unverified_https_context = ssl._create_unverified_context\n",
        "# except AttributeError:\n",
        "#     pass\n",
        "# else:\n",
        "#     ssl._create_default_https_context = _create_unverified_https_context\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words_id = set(stopwords.words('indonesian'))\n",
        "\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "stemmer_factory = StemmerFactory()\n",
        "stemmer = stemmer_factory.create_stemmer()\n",
        "\n",
        "print(\"Memulai proses persiapan data dan pelatihan model...\")\n",
        "\n",
        "# --- Fungsi Pra-pemrosesan Teks ---\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words_id]\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Fungsi Standarisasi Nama untuk Penggabungan\n",
        "def standardize_name_for_merge(name):\n",
        "    if pd.isna(name):\n",
        "        return ''\n",
        "    name = str(name).lower()\n",
        "    name = re.sub(r'[^\\w\\s]', '', name)\n",
        "    name = re.sub(r'\\s+', ' ', name).strip()\n",
        "    return name\n",
        "\n",
        "# --- BAGIAN 1: MUAT DAN PRA-PEMROSESAN DATASET ---\n",
        "\n",
        "# 1.1 Muat Dataset Pertama (Cleaned_data.csv) menjadi df\n",
        "try:\n",
        "    df = pd.read_csv('Cleaned_data.csv')\n",
        "    print(\"\\nDataset 'Cleaned_data.csv' berhasil dimuat ke df.\")\n",
        "\n",
        "    # Penanganan nilai hilang di kolom teks yang akan digabungkan\n",
        "    df['name'].fillna('', inplace=True)\n",
        "    df['description'].fillna('', inplace=True)\n",
        "    df['review_keywords'].fillna('', inplace=True)\n",
        "    df['address'].fillna('', inplace=True) # Untuk standarisasi place_name gabungan\n",
        "\n",
        "    # Buat kolom 'content' gabungan awal untuk Content-Based Filtering\n",
        "    df['content'] = df['name'] + ' ' + \\\n",
        "                            df['description'] + ' ' + \\\n",
        "                            df['review_keywords']\n",
        "    df['processed_content'] = df['content'].apply(preprocess_text)\n",
        "\n",
        "    print(\"Kolom konten awal untuk df berhasil diproses.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Pastikan file 'Cleaned_data.csv' ada di direktori yang sama.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Terjadi kesalahan saat memproses 'Cleaned_data.csv': {e}\")\n",
        "    exit()\n",
        "\n",
        "# 1.2 Muat Dataset Kedua (SentimentReview.csv) menjadi df_review\n",
        "try:\n",
        "    df_review = pd.read_csv('SentimentReview.csv')\n",
        "    print(\"\\nDataset 'SentimentReview.csv' berhasil dimuat ke df_review.\")\n",
        "\n",
        "    # Konversi 'published_at_date' ke datetime\n",
        "    df_review['published_at_datetime'] = pd.to_datetime(df_review['published_at_date'])\n",
        "\n",
        "    # Pastikan 'is_local_guide' menjadi integer (0/1)\n",
        "    df_review['is_local_guide'] = df_review['is_local_guide'].fillna(0).astype(int)\n",
        "\n",
        "    # Isi NaN pada 'text_akhir' dengan string kosong (teks yang sudah diproses)\n",
        "    df_review['text_akhir'].fillna('', inplace=True)\n",
        "\n",
        "    print(\"Kolom tanggal, is_local_guide, dan text_akhir di df_review berhasil diproses.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Pastikan file 'SentimentReview.csv' ada di direktori yang sama.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Terjadi kesalahan saat memproses 'SentimentReview.csv': {e}\")\n",
        "    exit()\n",
        "\n",
        "print(\"\\nKedua dataset telah dimuat dan diproses secara awal.\")\n",
        "\n",
        "# --- BAGIAN 2: PENGGABUNGAN DATASET BERDASARKAN place_name ---\n",
        "\n",
        "print(\"\\nMelakukan standarisasi 'place_name' dan membuat kunci gabungan untuk merge...\")\n",
        "\n",
        "# 2.1 Standarisasi di df (Dataset Pertama)\n",
        "df['name_standardized'] = df['name'].apply(standardize_name_for_merge)\n",
        "df['address_standardized'] = df['address'].apply(standardize_name_for_merge)\n",
        "df['merge_key'] = df['name_standardized'] + '_' + df['address_standardized']\n",
        "\n",
        "# 2.2 Standarisasi di df_review (Dataset Kedua)\n",
        "df_review['place_name_standardized'] = df_review['place_name'].apply(standardize_name_for_merge)\n",
        "df_review['merge_key'] = df_review['place_name_standardized'] # Kunci gabungan hanya place_name\n",
        "\n",
        "print(\"Standarisasi nama dan pembuatan kunci gabungan selesai.\")\n",
        "\n",
        "# 2.3 Agregasi data ulasan dari df_review per tempat wisata\n",
        "df_reviews_agg = df_review.groupby('merge_key').agg(\n",
        "    avg_review_rating=('rating', 'mean'),\n",
        "    total_reviews_from_raw=('rating', 'count'),\n",
        "    avg_polarity_score=('polarity_score', 'mean'),\n",
        "    positive_reviews_count=('polarity', lambda x: (x == 'positive').sum()),\n",
        "    negative_reviews_count=('polarity', lambda x: (x == 'negative').sum()),\n",
        "    all_review_texts=('text_akhir', lambda x: ' '.join(x.dropna().astype(str)))\n",
        ").reset_index()\n",
        "\n",
        "print(\"Dataset ulasan diagregasi per tempat wisata.\")\n",
        "print(df_reviews_agg.head())\n",
        "\n",
        "# 2.4 Gabungkan df dengan df_reviews_agg\n",
        "df_merged = pd.merge(df, df_reviews_agg, on='merge_key', how='left')\n",
        "\n",
        "print(\"\\nPenggabungan dataset selesai.\")\n",
        "print(\"Informasi df_merged setelah penggabungan:\")\n",
        "df_merged.info()\n",
        "\n",
        "# 2.5 Finalisasi Fitur Konten untuk Content-Based Model\n",
        "# Isi NaN dari kolom baru setelah merge\n",
        "df_merged['avg_review_rating'].fillna(df_merged['avg_review_rating'].mean(), inplace=True)\n",
        "df_merged['total_reviews_from_raw'].fillna(0, inplace=True)\n",
        "df_merged['avg_polarity_score'].fillna(0, inplace=True)\n",
        "df_merged['positive_reviews_count'].fillna(0, inplace=True)\n",
        "df_merged['negative_reviews_count'].fillna(0, inplace=True)\n",
        "df_merged['all_review_texts'].fillna('', inplace=True)\n",
        "\n",
        "# Perbarui 'final_processed_content' (teks dari df + teks dari ulasan yang sudah bersih)\n",
        "df_merged['final_processed_content'] = df_merged['processed_content'] + ' ' + df_merged['all_review_texts']\n",
        "\n",
        "print(\"\\nFitur konten akhir (final_processed_content) untuk Content-Based model siap.\")\n",
        "\n",
        "# --- BAGIAN 3: PELATIHAN DAN PENYIMPANAN MODEL CONTENT-BASED ---\n",
        "\n",
        "print(\"\\nMelatih dan menyimpan model Content-Based...\")\n",
        "\n",
        "# 3.1 Inisialisasi dan latih TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.85)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df_merged['final_processed_content'])\n",
        "\n",
        "# 3.2 Hitung Cosine Similarity Matrix\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# 3.3 Buat mapping place_id ke index\n",
        "indices = pd.Series(df_merged.index, index=df_merged['place_id']).drop_duplicates()\n",
        "\n",
        "# 3.4 Simpan Model Content-Based dan metadata yang diperlukan\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "joblib.dump(cosine_sim_matrix, 'cosine_sim_matrix.pkl')\n",
        "joblib.dump(indices, 'place_indices_map.pkl')\n",
        "\n",
        "# --- PERBAIKAN DI SINI: HAPUS 'polarity' dari daftar kolom untuk df_places_metadata ---\n",
        "df_merged_metadata = df_merged[['place_id', 'name', 'description', 'reviews', 'rating', 'featured_image',\n",
        "                                'address', 'review_keywords', 'link', 'coordinates', 'avg_review_rating',\n",
        "                                'total_reviews_from_raw', 'avg_polarity_score',\n",
        "                                'positive_reviews_count', 'negative_reviews_count', 'final_processed_content', 'merge_key']]\n",
        "df_merged_metadata.to_pickle('df_places_metadata.pkl')\n",
        "\n",
        "print(\"Model Content-Based (TF-IDF Vectorizer, Cosine Similarity Matrix) dan metadata tempat wisata telah disimpan.\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 4: PERSIAPAN DAN PELATIHAN MODEL COLLABORATIVE FILTERING (TensorFlow Matrix Factorization) ---\n",
        "\n",
        "print(\"\\nMenyiapkan dan melatih model Collaborative Filtering (TensorFlow Matrix Factorization)...\")\n",
        "\n",
        "# 4.1 Siapkan data untuk TensorFlow\n",
        "cf_data_tf = df_review[['user_id', 'merge_key', 'rating']].copy()\n",
        "cf_data_tf.dropna(subset=['rating'], inplace=True)\n",
        "\n",
        "# Map user_id dan item_id (merge_key) ke integer kontigu\n",
        "user_ids = cf_data_tf['user_id'].unique()\n",
        "item_ids = cf_data_tf['merge_key'].unique()\n",
        "\n",
        "num_users = len(user_ids)\n",
        "num_items = len(item_ids)\n",
        "\n",
        "user_id_mapping = {id: i for i, id in enumerate(user_ids)}\n",
        "item_id_mapping = {id: i for i, id in enumerate(item_ids)}\n",
        "\n",
        "# Buat DataFrame dengan ID integer yang dipetakan\n",
        "cf_data_tf['user_encoded'] = cf_data_tf['user_id'].map(user_id_mapping)\n",
        "cf_data_tf['item_encoded'] = cf_data_tf['merge_key'].map(item_id_mapping)\n",
        "\n",
        "# Pisahkan data menjadi fitur (input) dan target (rating)\n",
        "x = cf_data_tf[['user_encoded', 'item_encoded']].values\n",
        "y = cf_data_tf['rating'].values\n",
        "\n",
        "# Pisahkan data menjadi training dan testing set\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4.2 Definisikan Model Matrix Factorization dengan Keras\n",
        "embedding_size = 50 # Ukuran embedding (bisa disesuaikan)\n",
        "\n",
        "user_input = keras.Input(shape=(1,), name='user_id')\n",
        "item_input = keras.Input(shape=(1,), name='item_id')\n",
        "\n",
        "# Embedding layer untuk user\n",
        "user_embedding = layers.Embedding(\n",
        "    input_dim=num_users,\n",
        "    output_dim=embedding_size,\n",
        "    input_length=1,\n",
        "    name='user_embedding'\n",
        ")(user_input)\n",
        "user_vec = layers.Flatten(name='user_vector')(user_embedding)\n",
        "\n",
        "# Embedding layer untuk item\n",
        "item_embedding = layers.Embedding(\n",
        "    input_dim=num_items,\n",
        "    output_dim=embedding_size,\n",
        "    input_length=1,\n",
        "    name='item_embedding'\n",
        ")(item_input)\n",
        "item_vec = layers.Flatten(name='item_vector')(item_embedding)\n",
        "\n",
        "# Dot product dari embedding user dan item untuk prediksi rating\n",
        "dot_product = layers.Dot(axes=1, name='dot_product')([user_vec, item_vec])\n",
        "\n",
        "# Optional: Tambahkan bias untuk user dan item (membantu akurasi)\n",
        "user_bias = layers.Embedding(num_users, 1, name='user_bias')(user_input)\n",
        "item_bias = layers.Embedding(num_items, 1, name='item_bias')(item_input)\n",
        "dot_product_with_bias = layers.Add()([dot_product, layers.Flatten()(user_bias), layers.Flatten()(item_bias)])\n",
        "\n",
        "# Output layer (aktivasi sigmoid untuk skala rating 1-5, lalu scaling)\n",
        "# Jika rating asli 1-5, sigmoid output 0-1, maka dikalikan (MAX_RATING - MIN_RATING) dan ditambah MIN_RATING\n",
        "min_rating = 1.0 # Sesuaikan dengan rating minimum Anda\n",
        "max_rating = 5.0 # Sesuaikan dengan rating maksimum Anda\n",
        "\n",
        "output = layers.Activation('sigmoid')(dot_product_with_bias) # Output range 0-1\n",
        "output = layers.Lambda(lambda x: x * (max_rating - min_rating) + min_rating)(output) # Scale to min_rating-max_rating\n",
        "\n",
        "model_cf = keras.Model(inputs=[user_input, item_input], outputs=output)\n",
        "\n",
        "# Kompilasi model (gunakan Mean Squared Error sebagai loss untuk rating)\n",
        "model_cf.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# 4.3 Latih Model\n",
        "history = model_cf.fit(\n",
        "    [x_train[:, 0], x_train[:, 1]], # Input user_encoded, item_encoded\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=10, # Sesuaikan jumlah epoch\n",
        "    validation_data=([x_val[:, 0], x_val[:, 1]], y_val),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 4.4 Simpan Model TensorFlow\n",
        "model_cf.save('cf_tf_matrix_factorization_model.h5')\n",
        "\n",
        "# Simpan juga mapping ID\n",
        "joblib.dump(user_id_mapping, 'user_id_mapping.pkl')\n",
        "joblib.dump(item_id_mapping, 'item_id_mapping.pkl')\n",
        "joblib.dump(user_ids, 'unique_user_ids.pkl') # Simpan daftar user ID asli\n",
        "joblib.dump(item_ids, 'unique_item_ids.pkl') # Simpan daftar item ID asli (merge_key)\n",
        "\n",
        "print(\"Model Collaborative Filtering (TensorFlow) telah disimpan.\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 5: MENYIAPKAN DATA TAMBAHAN UNTUK DEPLOYMENT ---\n",
        "\n",
        "# 5.1 Siapkan DataFrame Interaksi Pengguna-Item Lengkap (untuk lookup di API)\n",
        "df_user_interactions = df_review[['user_id', 'merge_key', 'rating', 'published_at_datetime', 'polarity_score', 'polarity']].copy()\n",
        "df_user_interactions.rename(columns={'merge_key': 'item_id'}, inplace=True)\n",
        "joblib.dump(df_user_interactions, 'df_user_interactions.pkl')\n",
        "print(\"\\nDataFrame interaksi pengguna-item (df_user_interactions) telah disimpan.\")\n",
        "\n",
        "\n",
        "# 5.2 Siapkan DataFrame Fitur Pengguna (untuk lookup profil pengguna di API)\n",
        "df_user_features = df_review.groupby('user_id').agg(\n",
        "    total_reviews_given=('rating', 'count'),\n",
        "    total_photos_uploaded=('total_number_of_photos_by_reviewer', 'max'),\n",
        "    is_local_guide=('is_local_guide', 'max'),\n",
        "    avg_user_rating=('rating', 'mean'),\n",
        "    avg_user_polarity_score=('polarity_score', 'mean')\n",
        ").reset_index()\n",
        "joblib.dump(df_user_features, 'df_user_features.pkl')\n",
        "print(\"DataFrame fitur pengguna (df_user_features) telah disimpan.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Proses Persiapan Data dan Pelatihan Model Selesai ---\")\n",
        "print(\"Anda sekarang memiliki file-file berikut yang siap untuk deployment:\")\n",
        "print(\"  - tfidf_vectorizer.pkl (Content-Based)\")\n",
        "print(\"  - cosine_sim_matrix.pkl (Content-Based)\")\n",
        "print(\"  - place_indices_map.pkl (Mapping ID tempat asli ke index DataFrame)\")\n",
        "print(\"  - df_places_metadata.pkl (Metadata tempat wisata yang diperkaya)\")\n",
        "print(\"  - cf_tf_matrix_factorization_model.h5 (Collaborative Filtering TensorFlow Model)\")\n",
        "print(\"  - user_id_mapping.pkl (Mapping ID pengguna asli ke ID integer model TF)\")\n",
        "print(\"  - item_id_mapping.pkl (Mapping ID item asli ke ID integer model TF)\")\n",
        "print(\"  - unique_user_ids.pkl (Daftar ID pengguna unik asli)\")\n",
        "print(\"  - unique_item_ids.pkl (Daftar ID item unik asli - merge_key)\")\n",
        "print(\"  - df_user_interactions.pkl (Riwayat interaksi pengguna)\")\n",
        "print(\"  - df_user_features.pkl (Profil fitur pengguna)\")\n",
        "\n",
        "print(\"\\nUntuk deployment, Anda akan memuat file-file ini ke dalam backend web Anda.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "X_a6ebagjAQa",
        "outputId": "545074c9-2b4b-47b7-d129-46094cf1a4f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-06-03 23:13:14.378666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748967194.391513  156823 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748967194.395793  156823 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1748967194.405558  156823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748967194.405571  156823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748967194.405572  156823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748967194.405573  156823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-06-03 23:13:14.408633: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai proses persiapan data dan pelatihan model...\n",
            "Error: Pastikan file 'Cleaned_data.csv' ada di direktori yang sama.\n",
            "Error: Pastikan file 'SentimentReview.csv' ada di direktori yang sama.\n",
            "\n",
            "Kedua dataset telah dimuat dan diproses secara awal.\n",
            "\n",
            "Melakukan standarisasi 'place_name' dan membuat kunci gabungan untuk merge...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMelakukan standarisasi \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplace_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dan membuat kunci gabungan untuk merge...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# 2.1 Standarisasi di df (Dataset Pertama)\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_standardized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(standardize_name_for_merge)\n\u001b[1;32m    114\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress_standardized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(standardize_name_for_merge)\n\u001b[1;32m    115\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerge_key\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_standardized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress_standardized\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fe6j-eXmjZAD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}